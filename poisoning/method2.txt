METHOD 2: ADVERSARIAL ATTACKS (FGSM & PGD)
======================================================================

ATTACK DESCRIPTION
----------------------------------------------------------------------
This attack generates adversarial examples that are visually similar
to original images but are misclassified by the model.
Two methods are used:
1. FGSM (Fast Gradient Sign Method): Single-step attack
2. PGD (Projected Gradient Descent): Iterative multi-step attack

BASELINE MODEL PERFORMANCE (Clean Test Data)
----------------------------------------------------------------------
Test Accuracy: 99.21%
Correct Predictions: 9921/10000

FGSM ATTACK RESULTS
----------------------------------------------------------------------
Attack Parameters:
  Epsilon: 0.3
  Method: Single-step gradient sign perturbation

Results:
  Accuracy on Adversarial Examples: 81.54%
  Correct Predictions: 8154/10000
  Incorrect Predictions: 1846/10000
  Attack Success Rate: 18.46%
  Accuracy Drop: 17.67%

PGD ATTACK RESULTS
----------------------------------------------------------------------
Attack Parameters:
  Epsilon: 0.3
  Alpha (step size): 0.01
  Iterations: 40
  Method: Multi-step projected gradient descent

Results:
  Accuracy on Adversarial Examples: 53.17%
  Correct Predictions: 5317/10000
  Incorrect Predictions: 4683/10000
  Attack Success Rate: 46.83%
  Accuracy Drop: 46.04%

COMPARISON
----------------------------------------------------------------------
Method          Accuracy        Attack Success       Accuracy Drop
Clean           99.21           N/A                  0.00%
FGSM            81.54           18.46                17.67%
PGD             53.17           46.83                46.04%

PER-DIGIT ANALYSIS - FGSM
----------------------------------------------------------------------
Digit    Total    Correct    Accuracy     Most Predicted As
----------------------------------------------------------------------
0        100      88         88.00        0
1        100      91         91.00        1
2        100      78         78.00        2
3        100      93         93.00        3
4        100      61         61.00        4
5        100      83         83.00        5
6        100      71         71.00        6
7        100      65         65.00        7
8        100      82         82.00        8
9        100      77         77.00        9

PER-DIGIT ANALYSIS - PGD
----------------------------------------------------------------------
Digit    Total    Correct    Accuracy     Most Predicted As
----------------------------------------------------------------------
0        100      51         51.00        0
1        100      65         65.00        1
2        100      48         48.00        2
3        100      79         79.00        3
4        100      27         27.00        9
5        100      49         49.00        5
6        100      48         48.00        6
7        100      27         27.00        9
8        100      53         53.00        8
9        100      50         50.00        9

CONFUSION MATRIX - FGSM ATTACK
----------------------------------------------------------------------
          0      1      2      3      4      5      6      7      8      9 
  0     854      0     32      6      3      9     27      0     33     16 
  1       0    985     72     12      6      3      8      9     36      4 
  2       4      8    803    154      5      1      3     35     19      0 
  3       0      0     14    951      0     27      0      9      4      5 
  4       2      9     17      4    662      5      8      1     36    238 
  5       1      1      1     96      0    729      5      1     30     28 
  6      23      4      7      2     13     74    794      0     40      1 
  7       2     13     69     55     10      3      0    745      9    122 
  8       2      1     19     49      9     31      4      3    822     34 
  9       3      4      2     22     59     36      0     12     62    809 

CONFUSION MATRIX - PGD ATTACK
----------------------------------------------------------------------
          0      1      2      3      4      5      6      7      8      9 
  0     604      0    136     11     10     18     56      0     98     47 
  1       0    572    342     30     27      5     15     36    100      8 
  2       5     26    476    377      6      3     11     77     50      1 
  3       1      2     29    818      0    109      1     20     17     13 
  4       4     21     41      7    312      7     11      6     82    491 
  5       5      5      2    242      1    468     10      1     79     79 
  6      48      2     21      6     21    165    616      1     76      2 
  7       9     17    171    102     15     10      0    343     14    347 
  8       4      1     64    150     17     80      7      2    563     86 
  9       4      6     10     31    205     49      0     31    128    545 

INTERPRETATION
----------------------------------------------------------------------
Adversarial attacks exploit the gradient information of the model
to create imperceptible perturbations that fool the classifier.

FGSM: Fast single-step attack, achieved 18.46% success rate.
PGD: Stronger iterative attack, achieved 46.83% success rate.

PGD is typically more effective than FGSM as it uses multiple
iterations to find stronger adversarial perturbations.

These attacks demonstrate the vulnerability of neural networks to
carefully crafted adversarial examples, even when the perturbations
are small and often imperceptible to humans.
